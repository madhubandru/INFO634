{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFO 634- DATA MINING TERM PROJECT\n",
    "#### GROUP -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Madhu Bandru (mb4236)\n",
    "#### Vuthej (vv334)\n",
    "#### Likhil Rachuri (lkr46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing required packages. TensorFlow, pycocotools and also Compile protobufs and install the object_detection package\n",
    "#https://github.com/cocodataset/cocoapi/issues/169#issuecomment-462528628\n",
    "!pip install -U --pre tensorflow==\"2.*\"\n",
    "!pip install pycocotools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "#importing required packages in the below code.\n",
    "\n",
    "import cv2 \n",
    "import imutils \n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import numpy\n",
    "from numpy import linalg as la\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "\n",
    "\n",
    "#We can either download these models from tensorflow or clone them from the git.\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "    while \"models\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function to load the model, input: model name, Output: model\n",
    "\n",
    "def load_model(model_selection):\n",
    "    url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "    file_name = model_selection + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(fname=model_selection, origin=url + file_name, untar=True)\n",
    "    model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "    model = tf.saved_model.load(str(model_dir))\n",
    "    model = model.signatures['serving_default']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1, 'name': 'person'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the training the labels are required to download and initialize. set path for labels \"mscoco_label_map.pbtxt\" and creating the index accordingly\n",
    "## Confirming the \"person\" is at index 1.\n",
    "\n",
    "LABELS_PATH = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(LABELS_PATH, use_display_name=True)\n",
    "category_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# initializing the model. Loaded the coco model named 'ssd_mobilenet_v1_coco_2017_11_17'. This dataset is obtained from TensorFlow.\n",
    "\n",
    "model_name = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "detection_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'image_tensor:0' shape=(None, None, None, 3) dtype=uint8>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the inputs of the model laoded in detection_model\n",
    "detection_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_scores': tf.float32,\n",
       " 'detection_classes': tf.float32,\n",
       " 'num_detections': tf.float32,\n",
       " 'detection_boxes': tf.float32}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for the output produced by the detection_model\n",
    "# Dictionary with four elements are produced as output. detection_scores is of float datatype, defines the confidence score in percentage of the object identified\n",
    "# detection_classes is of float, defines objects of what classes are identified in the image\n",
    "# num_detections: defines the number of objects identified.\n",
    "#detection_boxes: Location of the objects identified in the image are given here with co-ordinates of 4 point array/list.\n",
    "detection_model.output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_scores': TensorShape([None, 100]),\n",
       " 'detection_classes': TensorShape([None, 100]),\n",
       " 'num_detections': TensorShape([None]),\n",
       " 'detection_boxes': TensorShape([None, 100, 4])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## important to know the output produced by the selected model.\n",
    "#detection scores, detection classes, detection boxes are of default size in 100. detection boxes are each of size 4.\n",
    "# num_detections are the exact number of objects identified. So, while we use the output of the model, we need to filter out the scores, classes\n",
    "# boxes according to the number in num_detections.\n",
    "detection_model.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below function is to identify the objects in an image using the TensorFlow DNN model loaded above.\n",
    "# Input: initialized model variable name, image, width of image, height of the image, threshold.\n",
    "## threshold is for the scores obtained from model output. objects passing the minimum threshold value are only considered further.\n",
    "### Threshold is set default to 0.35\n",
    "\n",
    "def obj_detection(model, image ,width,height,threshold=0.35):\n",
    "    #converting the image to numpy array\n",
    "    image = np.asarray(image)\n",
    "    # Converting the numpy array to tensor.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # Adding an axis to the tensor as model would be expecting n images. so adding empty dimension.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Pass the tensor obtained above to the model for output.\n",
    "    output_dict = model(input_tensor)\n",
    "    # collecting the output obtained from the model into boxes, their scores and classes.  \n",
    "    boxes = np.squeeze(output_dict['detection_boxes'])\n",
    "    scores = np.squeeze(output_dict['detection_scores'])\n",
    "    # converting as integer as labels would be only in integers.\n",
    "    classes = np.squeeze(output_dict['detection_classes']).astype(np.int32)\n",
    "    # Filtering the objects identified to get the persons label.\n",
    "    ## label \"person\" is with class 1.\n",
    "    ## collecting the indices who has class as 1.\n",
    "    indices = np.argwhere(classes == 1)\n",
    "    # filtering out the boxes,scores and classes for only those indices obtained above.\n",
    "    boxes = np.squeeze(boxes[indices], axis=1)\n",
    "    scores = np.squeeze(scores[indices], axis=1)\n",
    "    classes = np.squeeze(classes[indices], axis=1)\n",
    "    #setting a min threshold to 0.35 from the function parameters.\n",
    "    min_score_thresh = threshold\n",
    "    ## filtering out those objects obtained form model whose scores passed the min threshold value\n",
    "    bboxes = boxes[scores > min_score_thresh]\n",
    "    #obtained box values are in normalized form, so we need to multiply them with the image width and height to get the exact values.\n",
    "    im_width, im_height = (width , height)\n",
    "    final_box = []\n",
    "    #Converting Standardised values to normal values\n",
    "    for box in bboxes:\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        ## re-generating the boxes with regular format from normalized form.\n",
    "        final_box.append([xmin * im_height, ymin *im_width , xmax * im_height, ymax *im_width ])\n",
    "    # returning the final_box with coordinates of boxes.\n",
    "    return final_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core method to calculate the social distances.\n",
    "## input: cam_input_port is set default to zero, it is the port for the web camera in case of live recording.\n",
    "## path: Takes in the path for the video in which social distance is to be calculated\n",
    "## model: This function can be performed by two models. model 1: Using Hog, model 2: Using TF DNN model.\n",
    "def social_distance_monitoring(cam_input_port=0, path=\"\", model=2):\n",
    "    if path == \"\":\n",
    "        cv2.startWindowThread()\n",
    "        # opening the webcam for the live video input\n",
    "        cap = cv2.VideoCapture(cam_input_port)\n",
    "        # output is also written into a file. initializing the output file for live video\n",
    "        save_video = cv2.VideoWriter('Live_video_output.avi',cv2.VideoWriter_fourcc(*'MJPG'),15.,(640,480))\n",
    "    else:\n",
    "        cv2.startWindowThread()\n",
    "        # providing the path to cv2(opencv) to get the video data.\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        width = cap.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "        height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
    "        # output is also written into a file. initializing the output file for recorded video\n",
    "        save_video = cv2.VideoWriter('Recorded_video_output.avi',cv2.VideoWriter_fourcc(*'MJPG'),15.,(int(width),int(height)))\n",
    "    ## initializing the HOGDescriptor for hog based model.\n",
    "    if model == 1:\n",
    "        feature_descriptor = cv2.HOGDescriptor()\n",
    "        ## passing through a SVM detector.\n",
    "        feature_descriptor.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    #variables to count the total persons identified and total violations predicted.\n",
    "    human_count_in_video=0\n",
    "    total_violator_count=0\n",
    "    ## using an while loop to trigger the frames throughout the video.\n",
    "    while cap.isOpened():\n",
    "        # Reading the Video Stream input as frames, video is trimmed into frmaes and each frame is processed below.\n",
    "        ret, image = cap.read()\n",
    "        ## font styles and sizes and color for the text to write on the video\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "        fontScale = 1\n",
    "        color = (0, 255, 255)\n",
    "        ## Text position adjusted on the output video\n",
    "#         if ret:\n",
    "#             text_position_x = image.shape[1] - 550\n",
    "#         else:\n",
    "#             break\n",
    "        text_position_y=20\n",
    "        text_position_y2=50\n",
    "        ## thickness of the letters\n",
    "        thickness = 2\n",
    "        \n",
    "        ## Condition to check if the image is returned.\n",
    "        if ret: \n",
    "            text_position_x = image.shape[1] - 550\n",
    "            ## deciding which model to choose, here this block is for TensorFlow DNN model.\n",
    "            if model == 2:\n",
    "                ## input image is resized according to our customization.\n",
    "                image = imutils.resize(image,width=min(4000, image.shape[1]),height=min(4000, image.shape[0])) \n",
    "                # Detecting all the regions in the Image that has a pedestrians inside it \n",
    "                width=min(4000, image.shape[1])\n",
    "                height=min(4000, image.shape[0])\n",
    "                ## Image obtained from frame is passed into the model to get the locations of bounding boxes and also passing the required \n",
    "                ## parameters for the function, threshold for scores obtained from model is given as 0.35\n",
    "                regions1=obj_detection(detection_model,image ,width,height,0.35)\n",
    "                ## converting the bounding box coordinates to near by integers as we would get output in float and below code requires it in int.\n",
    "                regions=[]\n",
    "                for i in regions1:\n",
    "                    regions.append([int(j) for j in i])\n",
    "                ## count of 'person' objects detected in each frame is stored into variable named human_count_in_frame\n",
    "                human_count_in_frame=len(regions)\n",
    "                ## initializing the violator and non-violator counts to zero for the frame.\n",
    "                frame_violator_count=0\n",
    "                frame_non_violator_count=0\n",
    "\n",
    "                # Calculating the distance between two objects. \n",
    "                for person in regions:\n",
    "                    ## calculating the person's center. So, that distance between two points can be calculated\n",
    "                    person_center = numpy.array([(person[1] + int((person[3] - person[1])/2), person[0] + int((person[2] - person[0])/2))])\n",
    "                    person_violated = False\n",
    "                    ## checking the distance from this point to all the other points\n",
    "                    for other_person in regions:\n",
    "                        ##ignoring the self check\n",
    "                        if list(person) != list(other_person):\n",
    "                            ## calculating the center of the other object identified\n",
    "                            other_person_center = numpy.array([(other_person[1] + int((other_person[3] - other_person[1])/2), other_person[0] + int((other_person[2] - other_person[0])/2))])\n",
    "                            ## calculating the distance by \"EUCLIDEAN DISTANCE\".\n",
    "                            ### checking whether the distance is following the minimum threshold\n",
    "                            ## threshold is 'person[3]' gives us the height which is assumed to be 6 feet and calculating the social distancing based on that.\n",
    "                            if (la.norm(person_center - other_person_center)) < (person[2] - person[0]):\n",
    "                                ## setting the violated flag to true for the object as it doesnt maintain distance with threshold given\n",
    "                                person_violated = True\n",
    "                                break\n",
    "                    ## if person/object violated the distance, plotting the red colored rectangular boxes around the object.\n",
    "                    if person_violated:\n",
    "                        ## plotting the rectangle around the object\n",
    "                        cv2.rectangle(image, (person[1], person[0]), (person[3], person[2]), (0, 0, 255), 2)\n",
    "                        ## finding the center and plotting the centers\n",
    "                        cv2.circle(image, (person[1] + int((person[3] - person[1])/2), person[0] + int((person[2] - person[0])/2)), 1, (0, 0, 255), 10)\n",
    "                        ## incrementing the vioaltors count as 'person_violated' flag is triggered\n",
    "                        frame_violator_count+=1\n",
    "                    ## case if social distance is followed\n",
    "                    else:\n",
    "                        ## plotting the green rectangles around the object\n",
    "                        cv2.rectangle(image, (person[1], person[0]), (person[3], person[2]), (0,255, 0), 2)\n",
    "                        ## plotting the centers of the rectangle on the image frame\n",
    "                        cv2.circle(image, (person[1] + int((person[3] - person[1])/2), person[0] + int((person[2] - person[0])/2)), 1, (0,255, 0), 10)\n",
    "                        ## non-violator count is increased.\n",
    "                        frame_non_violator_count+=1\n",
    "                ## appending the human objects identified in the frame to the total number calculated throughout the video.\n",
    "                human_count_in_video+=human_count_in_frame\n",
    "                total_violator_count+=frame_violator_count\n",
    "                ## setting the text for violator count in the frame.\n",
    "                text1=\"Violaters in the Frame: %d\" %frame_violator_count\n",
    "                ## to avoid divide by zero error if no object is identified in the first frame. human_count_in_video is set to 1, if no objects are identified.\n",
    "                if human_count_in_video==0:\n",
    "                    human_count_in_video=1\n",
    "                ## setting the text to print the violations percentage on the video.\n",
    "                text2=\"Overall violations percentage: %.2f\" %(total_violator_count/human_count_in_video)\n",
    "                ## printing the text for violators count and percentage on each frame using the opencv\n",
    "                cv2.putText(image, text1, (text_position_x,text_position_y), font, fontScale,color, thickness, cv2.LINE_AA, False)\n",
    "                cv2.putText(image, text2, (text_position_x,text_position_y2), font, fontScale,color, thickness, cv2.LINE_AA, False)\n",
    "                ## writing the each frame into the output video.\n",
    "                save_video.write(image)\n",
    "                # Showing the output Image \n",
    "                cv2.imshow(\"Image\", image) \n",
    "                ## setting the waitkey to pull back if struck.\n",
    "                if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "                    break\n",
    "            ## if the model is hog model from opencv. This is based on intensities.\n",
    "            elif model == 1:\n",
    "                ## image is resized accordingly.\n",
    "                image = imutils.resize(image, width = min(4000, image.shape[1])) \n",
    "                ## each frame from the video is taken as image and fed to the feature descriptor to identify the objects and bounding boxes\n",
    "                (regions, _) = feature_descriptor.detectMultiScale(image, winStride=(4, 4), padding=(4, 4), scale=1.05) \n",
    "                ## total objects identified in the frame are the length of the output returned from the model.\n",
    "                human_count_in_frame=len(regions)\n",
    "                ## counts for violator and non-violator are set to zero.\n",
    "                frame_violator_count=0\n",
    "                frame_non_violator_count=0\n",
    "                # Calculating the social distance\n",
    "                for person in regions:\n",
    "                    ## calculating the centre of object identified\n",
    "                    ## output coordinates obtained from hog and TF DNN model are different so calculations are different.\n",
    "                    person_center = numpy.array([int(person[0] + (person[2]/2)), int(person[1] + (person[3]/2))])\n",
    "                    ## initializing the flag to False\n",
    "                    person_violated = False\n",
    "                    for other_person in regions:\n",
    "                        if list(person) != list(other_person):\n",
    "                            ## calculating the center for other objects\n",
    "                            other_person_center = numpy.array([int(other_person[0] + (other_person[2]/2)), int(other_person[1] + (other_person[3]/2))])\n",
    "                            ## calculating the distance between the objects and checking whether the distance is less than the threshold\n",
    "                            ## threshold is set to the height of the box assuming it to be 6 feet.\n",
    "                            ## person[3] is height of the box.\n",
    "                            if (la.norm(person_center - other_person_center)) < person[3]:\n",
    "                                ## triggering the flag to TRUE\n",
    "                                person_violated = True\n",
    "                                break\n",
    "                    ## visualizations if person has violated the social distance\n",
    "                    if person_violated:\n",
    "                        ## plotting the rectangle  box around the object with red color as person is violating the social distance\n",
    "                        cv2.rectangle(image, (person[0], person[1]), (person[0] + person[2], person[1] + person[3]), (0, 0, 255), 2)\n",
    "                        ## plotting the center of the person\n",
    "                        cv2.circle(image, (int(person[0] + (person[2]/2)), int(person[1] + (person[3]/2))), 1, (0, 0, 255), 10)\n",
    "                        ## increasing the count of violators\n",
    "                        frame_violator_count+=1\n",
    "                    ## visualizations for the objects identified and observing the social distance between them.\n",
    "                    else:\n",
    "                        cv2.rectangle(image, (person[0], person[1]), (person[0] + person[2], person[1] + person[3]), (0, 255, 0), 2)\n",
    "                        cv2.circle(image, (int(person[0] + (person[2]/2)), int(person[1] + (person[3]/2))), 1, (0, 255, 0), 10)\n",
    "                        ## incrementing the non-violators count\n",
    "                        frame_non_violator_count+=1\n",
    "                ##appending the counts obtained in this frame to the total obtained over the video\n",
    "                human_count_in_video+=human_count_in_frame\n",
    "                total_violator_count+=frame_violator_count\n",
    "                ## setting the text for the violator count\n",
    "                text1=\"Violaters in the Frame: %d\" %frame_violator_count\n",
    "                ## to avoid zero division error, human_count_in_video is set to 1,\n",
    "                if human_count_in_video==0:\n",
    "                    human_count_in_video=1\n",
    "                ## setting text for violators percentage value\n",
    "                text2=\"Overall violations percentage: %.2f\" %(total_violator_count/human_count_in_video)\n",
    "                ## printing the text on to the image frame\n",
    "                cv2.putText(image, text1, (text_position_x,text_position_y), font, fontScale,color, thickness, cv2.LINE_AA, False)\n",
    "                cv2.putText(image, text2, (text_position_x,text_position_y2), font, fontScale,color, thickness, cv2.LINE_AA, False)\n",
    "                ## writing the frame into the output video\n",
    "                save_video.write(image)\n",
    "                ## displaying the image frame\n",
    "                cv2.imshow(\"Image\", image) \n",
    "                ## setting the wait key to pull back if struck\n",
    "                if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        else: \n",
    "            break\n",
    "    # releasing the capture, when all the video is completed.\n",
    "    cap.release()\n",
    "    # save_video.release, finishes writing onto video and releases it.\n",
    "    save_video.release()\n",
    "    ## closes the windows opened.\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please select a model (hog_model or tf),  hog_model\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import imutils \n",
    "import numpy\n",
    "## cam_input_port is set to zero, for webcam\n",
    "cam_input_port=0\n",
    "## path to be provided for the input video\n",
    "video_input_path=\"vid_short.mp4\"\n",
    "##selecting the model\n",
    "model={\"hog_model\": 1, \"tf\": 2}\n",
    "## user selection for the model\n",
    "user_input=input(\"Please select a model (hog_model or tf), \")\n",
    "## calling the function to calculate the social distancing\n",
    "social_distance_monitoring(cam_input_port, path=video_input_path, model= model[user_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
